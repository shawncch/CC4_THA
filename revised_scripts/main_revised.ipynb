{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# run this once to write json file\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Papagoat/brain-assessment/main/restaurant_data.json\"\n",
    "response = requests.get(url)\n",
    "with open('../source/restaurant_data.json', 'w') as json_file:  \n",
    "    json.dump(response.json(), json_file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data():  #initialize json data and populate missing zomato_events keys\n",
    "    with open(\"../source/restaurant_data.json\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for i, result in enumerate(data):\n",
    "        for j, restaurant in enumerate(result['restaurants']):\n",
    "            if 'zomato_events' not in restaurant['restaurant']:\n",
    "                data[i][\"restaurants\"][j][\"restaurant\"][\"zomato_events\"] = []\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_unique_restaurants(data):\n",
    "\n",
    "    #time complexity of O(n*d), where n is total no. of restaurants and d is the maximum depth we retrieve our value from\n",
    "    unique_restaurants_df = pd.json_normalize(data, record_path=['restaurants'])  \n",
    "\n",
    "    unique_restaurants_df = unique_restaurants_df[[\n",
    "                        \"restaurant.R.res_id\",\n",
    "                        \"restaurant.name\", \n",
    "                        \"restaurant.location.country_id\", \n",
    "                        \"restaurant.location.city\",\n",
    "                        \"restaurant.user_rating.votes\",\n",
    "                        \"restaurant.user_rating.aggregate_rating\",\n",
    "                        \"restaurant.user_rating.rating_text\",\n",
    "                        \"restaurant.cuisines\"\n",
    "                        ]]\n",
    "\n",
    "    rename_dict = {\"restaurant.name\" : \"Restaurant Name\", \n",
    "                    \"restaurant.R.res_id\" : \"Restaurant Id\",\n",
    "                    \"restaurant.location.country_id\" : \"Country Code\", \n",
    "                    \"restaurant.location.city\" : \"City\",\n",
    "                    \"restaurant.user_rating.votes\" : \"User Rating Votes\",\n",
    "                    \"restaurant.user_rating.aggregate_rating\" : \"User Aggregate Rating\",\n",
    "                    \"restaurant.user_rating.rating_text\" : \"User Rating Text\",\n",
    "                    \"restaurant.cuisines\" : \"Cuisines\"}\n",
    "\n",
    "    unique_restaurants_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    unique_restaurants_df['User Aggregate Rating'] = unique_restaurants_df['User Aggregate Rating'].astype('float')\n",
    "    \n",
    "\n",
    "    return unique_restaurants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_unique_events(data):\n",
    "        unique_events_df = pd.json_normalize(data, \n",
    "                                                record_path=['restaurants', 'restaurant', 'zomato_events'], \n",
    "                                                meta = [[\"restaurants\", \"restaurant\", \"R\", \"res_id\"], [\"restaurants\", \"restaurant\", \"name\"]])\n",
    "\n",
    "        unique_events_df[\"photo_urls\"] = unique_events_df[\"event.photos\"].apply(lambda x: [photo[\"photo\"][\"url\"] for photo in x])\n",
    "\n",
    "\n",
    "        unique_events_df = unique_events_df[[\n",
    "                                        \"event.event_id\",\n",
    "                                        \"restaurants.restaurant.R.res_id\",\n",
    "                                        \"restaurants.restaurant.name\",\n",
    "                                        \"photo_urls\",\n",
    "                                        \"event.title\",\n",
    "                                        \"event.start_date\",\n",
    "                                        \"event.end_date\",\n",
    "                                        \n",
    "                                ]]\n",
    "\n",
    "        rename_dict = {\n",
    "                        \"event.event_id\" : \"Event Id\",\n",
    "                        \"restaurants.restaurant.R.res_id\" : \"Restaurant Id\",\n",
    "                        \"restaurants.restaurant.name\" : \"Restaurant Name\",\n",
    "                        \"event.title\" : \"Event Title\",\n",
    "                        \"event.start_date\" : \"Event Start Date\",\n",
    "                        \"event.end_date\" : \"Event End Date\",\n",
    "                        \"photo_urls\" : \"Photo URL List\"\n",
    "                        }\n",
    "\n",
    "        unique_events_df.rename(columns=rename_dict, inplace=True)\n",
    "        unique_events_df[\"Photo URL List\"] = unique_events_df[\"Photo URL List\"].apply(lambda x: \"NA\" if len(x) == 0 else x)\n",
    "        unique_events_df[\"Restaurant Id\"] = unique_events_df[\"Restaurant Id\"].astype('int')\n",
    "        \n",
    "\n",
    "        return unique_events_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_country_codes():\n",
    "    country_code_df = pd.read_excel('../source/Country-Code.xlsx', sheet_name='Sheet1')\n",
    "    return country_code_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_restaurant_data(unique_restaurants_df, country_code_df):\n",
    "    merged_df = unique_restaurants_df.merge(country_code_df, on = \"Country Code\", how = \"inner\")\n",
    "    merged_df = merged_df[[\"Restaurant Id\", \"Restaurant Name\", \"Country\", \"City\", \"User Rating Votes\", \"User Aggregate Rating\", \"Cuisines\"]]\n",
    "    merged_df = merged_df.fillna(\"NA\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_restaurant_data_with_events(merged_restaurants_df, unique_events_df): # restaurant details duplicated depending on no. of events\n",
    "    unique_events_df.drop(columns = [\"Restaurant Name\"], inplace = True) #drop duplicate column\n",
    "    merged_df = merged_restaurants_df.merge(unique_events_df, on = \"Restaurant Id\", how = \"left\")\n",
    "    merged_df = merged_df.fillna(\"NA\")\n",
    "\n",
    "    # removing trailing zeros from event id, trailing zeros appeared due to the merge \n",
    "    # pandas automatically promotes the entire column’s data type to float to accommodate null values\n",
    "    merged_df['Event Id'] = merged_df['Event Id'].apply(lambda x: \"NA\" if x == \"NA\" else int(x))\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_restaurant_events(restaurant_data_df):\n",
    "    filtered_df = restaurant_data_df[(restaurant_data_df[\"Event Start Date\"] != \"NA\") & (restaurant_data_df[\"Event End Date\"] != \"NA\")]\n",
    "    filtered_df[\"converted_eventstartdt\"] = pd.to_datetime(filtered_df[\"Event Start Date\"])\n",
    "    filtered_df[\"converted_eventenddt\"] = pd.to_datetime(filtered_df[\"Event End Date\"])\n",
    "\n",
    "    april_2019 = pd.to_datetime(\"2019-04-01\")\n",
    "\n",
    "    filtered_df1 = filtered_df[(filtered_df[\"converted_eventstartdt\"] <= april_2019) & (filtered_df[\"converted_eventenddt\"] >= april_2019)]\n",
    "    filtered_df2 = filtered_df1[[\"Event Id\", \"Restaurant Id\", \"Restaurant Name\", \"Photo URL List\", \"Event Title\", \"Event Start Date\", \"Event End Date\"]]\n",
    "    \n",
    "    return filtered_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. \tFrom the dataset (restaurant_data.json), determine the threshold for the different rating text based on aggregate rating. Return aggregates for the following ratings only:  \n",
    "\n",
    "◦   \tExcellent  \n",
    "◦   \tVery Good  \n",
    "◦   \tGood  \n",
    "◦   \tAverage  \n",
    "◦   \tPoor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_thresholds(data):\n",
    "    restaurant_data_df = preprocess_unique_restaurants(data)\n",
    "    restaurant_data_df = restaurant_data_df.loc[lambda x : x[\"User Rating Text\"].isin([\"Excellent\", \"Very Good\", \"Good\", \"Average\", \"Poor\"])]\n",
    "    thresholds = restaurant_data_df.groupby(\"User Rating Text\").agg(\n",
    "        min_rating=(\"User Aggregate Rating\", \"min\"),\n",
    "        max_rating=(\"User Aggregate Rating\", \"max\"),\n",
    "        avg_rating=(\"User Aggregate Rating\", \"mean\")\n",
    "    )\n",
    "\n",
    "    thresholds.sort_values(by = \"avg_rating\", inplace = True)\n",
    "\n",
    "    print(thresholds)\n",
    "\n",
    "    thresholds.to_csv(\"../output/thresholds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  min_rating  max_rating  avg_rating\n",
      "User Rating Text                                    \n",
      "Poor                     2.2         2.2    2.200000\n",
      "Average                  2.5         3.4    3.193333\n",
      "Good                     3.5         3.9    3.776224\n",
      "Very Good                4.0         4.4    4.215891\n",
      "Excellent                4.5         4.9    4.666207\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = initialize_data()\n",
    "    unique_restaurants_df = preprocess_unique_restaurants(data)\n",
    "    unique_events_df = preprocess_unique_events(data)\n",
    "    country_code_df = preprocess_country_codes()\n",
    "    \n",
    "    merged_unique_restaurants_df = output_restaurant_data(unique_restaurants_df, country_code_df)\n",
    "    filtered_unique_events_df = output_restaurant_events(unique_events_df)\n",
    "    restaurants_with_events_df = output_restaurant_data_with_events(merged_unique_restaurants_df, unique_events_df)\n",
    "\n",
    "    # Requirement 1\n",
    "    merged_unique_restaurants_df.to_csv(\"../output/restaurant_details.csv\", index = False)\n",
    "    restaurants_with_events_df.to_csv(\"../output/restaurant_withevents.csv\", index = False)\n",
    "    \n",
    "    # Requirement 2\n",
    "    filtered_unique_events_df.to_csv(\"../output/restaurant_events.csv\", index = False)\n",
    "\n",
    "    # Requirement 3: Ratings threshold\n",
    "    output_thresholds(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the aggregated results after grouping by the rating texts, an appropriate threshold for each rating text would be as follows:  \n",
    "\n",
    "Poor: 2.2 <= rating < 2.5  \n",
    "Average: 2.5 <= rating < 3.5  \n",
    "Good: 3.5 <= rating < 4.0  \n",
    "Very Good: 4.0 <= rating < 4.5  \n",
    "Excellent: Above 4.5  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
